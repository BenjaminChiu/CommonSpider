# 模型调优

## Batch Normalization，批量归一化

https://www.cnblogs.com/guoyaohua/p/8724433.html

BN的求梯度 在forward（前向传播）阶段就已经完成的。 只是accumulation_steps=8和真实的batchsize放大八倍相比，效果自然是差一些，毕竟八倍Batchsize的BN估算出来的均值和方差肯定更精准一些。

bn自己有个momentum参数：x_new_running = (1 - momentum) * x_running + momentum * x_new_observed. momentum越接近0，老的running stats记得越久，所以可以得到更长序列的统计信息。

N类里面momentum这个属性默认为0.1，可以尝试调低BN自己的momentum参数

## 参数与超参数

- 超参数：定义模型结构、优化策略（主要依靠人的经验设定）
	- 聚类算法中的类别个数
	- 训练中批量大小
	- 梯度下降法中的步长
	- 正则化项的系数
	- 神经网络的层数
	- 支持向量机中的核函数
- 参数：可通过优化算法学习的
	- 权重
	- 偏置值

## 模型调优

1. 参数优化：权值（W） 截断正态分布，偏置值初始化为0 + 0.1；批次容量（batch_size），训练迭代次数（21？以收敛程度为根据）
2. 神经元层数，单层个数，dropout 系数，神经元中的激活函数
3. 优化器

样本大小 和 模型复杂程度 成正相关，这其中可以用drop_out来调整模型工作的神经元个数

更强的计算性能，更多的数据，以及更好的训练方法。

## 权值计算

## 过拟合

概念：在训练中，对训练数据过度拟合，导致在实际预测中，远远不能达到训练时的准确率  
例如：一个学生平时做题很准确，但一到考试就拉垮

#### 模型过拟合，怎么办？

1. 权重衰减 让神经元上的权重 变得小一点，从而达到降低模型拟合能力的效果
2. 倒置丢弃法 丢弃掉部分隐藏层的神经元 测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。丢弃法只在训练模型时使用。

为了避免过拟合，最常用的一种方法是使用使用正则化，例如 L1 和 L2 正则化。

## 梯度累积

------

```python
链接：https: // www.zhihu.com / question / 303070254 / answer / 573037166

for i, (images, target) in enumerate(train_loader):
	# 1. input output
	images = images.cuda(non_blocking=True)
	target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)
	outputs = model(images)
	loss = criterion(outputs, target)

	# 2.1 loss regularization
	loss = loss / accumulation_steps
	# 2.2 back propagation
	loss.backward()
	# 3. update parameters of net
	if ((i + 1) % accumulation_steps) == 0:
		# optimizer the net
		optimizer.step()  # update parameters of net
		optimizer.zero_grad()  # reset gradient

1.
获取loss：输入图像和标签，通过infer计算得到预测值，计算损失函数；
2.
loss.backward()
反向传播，计算当前梯度；
3.
多次循环步骤1 - 2，不清空梯度，使梯度累加在已有梯度上；
4.
梯度累加了一定次数后，先optimizer.step()
根据累计的梯度更新网络参数，然后optimizer.zero_grad()
清空过往梯度，为下一波梯度累加做准备；


```

**梯度累加 目的**：实现低显存跑大batchsize

**梯度累加 概念**：就是每次获取1个batch的数据，计算1次梯度，梯度不清空，不断累加，累加一定次数后，根据累加的梯度更新网络参数，然后清空梯度（梯度清零），进行下一次循环。

一定条件下，batchSize越大训练效果越好，梯度累加则实现了batchSize的变相扩大。 如果accumulation_steps为8，则batchSize '变相' 扩大了8倍，是我们这种乞丐实验室解决显存受限的一个不错的trick，使用时需要注意，学习率也要适当放大。

注意 梯度归零 需要在 反向传播前







