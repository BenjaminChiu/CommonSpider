> 原文地址 https://blog.csdn.net/weixin_41036461/article/details/80657929

## 前言

一般来说，卷积神经网络会有三种类型的隐藏层——卷积层、池化层、全连接层。卷积层和池化层比较好理解，主要很多教程也会解释。

* 卷积层 (Convolutional layer) 主要是用一个采样器从输入数据中采集关键数据内容；
* 池化层 (Pooling layer) 则是对卷积层结果的压缩得到更加重要的特征，同时还能有效控制过拟合。

但是可怜的全连接层 (Fully Connected layer) 很多时候都被忽略了，可能大佬们觉得我们都能懂吧。。查阅了一下资料，大概理解了全连接层的作用，即将前面经过多次卷积后高度抽象化的特征进行整合，然后可以进行归一化，对各种分类情况都输出一个概率，之后的分类器 (Classifier) 可以根据全连接得到的概率进行分类。
这是我理解过后的总结，如有不当之后也敬请指正。当然结合了国内外很多篇文章才最终大概理解了全连接层的作用。最近又沉迷翻译，这篇文章就准备翻译下 stackexchange 里面关于 CNN 中 FC layer 的作用。 水平有限，欢迎各位指正。

## 卷积神经网络中全连接层作用 (What do the fully connected layers do in CNNs?)

Question: 我理解了卷积层和池化层的作用，但是我不能理解卷积神经网络中全连接层的作用。为什么不能将前一层直接输出到输出层呢？

Answer: 卷积层的输出代表着数据的高级特征。当输出可以被扁平化并且能够被连接到输出层时，添加一个全连接层通常能以简易的方式学习到这些非线性组合特征。 实质上，卷积层提供了一个`有意义、低维度且几乎不变的特征空间`，然后全连接层在这个空间里学习一个（非线性）方程。 注：从全连接层转换为卷积层是很方便的。将这些顶层全连接层转换为卷积层是很有帮助的。

Q: 所以我们通过反向传播来学习连接层之间的权重，这是否正确？

A: 是的，错误通过反向传播从全连接层传播到卷积层和池化层。

Q: 所以，全连接层的目的就是就像 PCA(主成分分析)，它通过学习全部的权重来整合了 “好” 的特征并减少其他特征。

A: 这主要取决于你特征的非线性组合。所有的特征都可能是好的（假设你没有死掉的特征），那么这些特征的组合就可能更好。