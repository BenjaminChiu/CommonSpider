# 卷积神经网络

----------

## 卷积神经网络（CNN,Convolutional Neural Networks）

### 发源

由生物学上的**感受野**机制的启发而提出的。 视网膜上的光感受器，接受刺激而兴奋时，将神经冲动信号传到视觉皮层，但不是所有视觉皮层都会接受这些信号，只有特定的、与这个光感受器相关联的视觉皮层神经元才会信号。

	局部 感受野 ==> 神经认知机（被看作 卷积神经网络的第一个实现网络，也是感受野概念在人工神经网络领域的首次应用）

### 结构特性，为什么要用卷积神经网络？

在图像处理领域，前馈神经网络的劣势:

1. 参数太多
2. 局部不变性特征，前馈神经网络很难提取

卷积神经网络特性：

- 局部连接
- 权重共享
- 汇聚

这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性，且模型参数少。  
卷积神经网络是一种深层前馈神经网络。

解决办法（how）： 只看有特征的一部分

滤波器就是模型权重，滤波器不变、权重不变

### 什么是参数共享机制？（权重共享）

数据窗口滑动，导致输入在变化，但中间滤波器Filter w0的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即所谓的CNN中的参数（权重）共享机制。

### CNN结构

卷积层、汇聚层、全连接层交叉堆叠而成的前馈神经网络。  
使用反向传播算法进行训练。

![](./img/CNN/02.png)

### 卷积运算

卷积核： 一个有固定的权重的矩阵，也被称为滤波器、滤波矩阵、filter

将原始图像（输入图像）的局部窗口 与 卷积核 按位相乘（进行 卷积/内积 运算），这样就会得到一个新的矩阵。

这个输出的 新的矩阵（二维数组）被称为特征图（可以理解为 被卷积核（滤波器）处理后得到的一种特征、表征）

#### 详细解释：

0.（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器）  
1.卷积是 矩阵的内积运算，不是矩阵乘法。即相同位置的数字相乘再相加求和  
2.卷积核也叫滤波器，滤波器有均值滤波器，高斯滤波器，拉普拉斯滤波器等等，所有的滤波器都只是一种数学运算  
3.每一层的卷积核大小和个数可以自己定义，不过一般情况下，根据实验得到的经验来看，会在越靠近输入层的卷积层设定少量的卷积核，越往后，卷积层设定的卷积核数目就越多。具体原因大家可以先思考一下，小结里会解释原因。

### 卷积神将网络 输出的计算公式为：

**N=(W-F+2P)/S+1**
其中N：输出大小 W：输入大小 F：卷积核大小 P：填充值的大小 S：步长大小 下面举个例子看一下：

```
   nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=2)

# 由公式可知，当需要卷积层输出特征_w == 输入特征_w，填充值(pading)=1，步长=1时
# 卷积核大小 = 3
```

卷积一层的几个参数:
in_channels=3:表示的是输入的通道数，由于是RGB型的，所以通道数是3. out_channels=96:表示的是输出的通道数，设定输出通道数的96（这个是可以根据自己的需要来设置的） kernel_size=12:表示卷积核的大小是12x12的，也就是上面的 “F”, F=12 stride=4:表示的是步长为4，也就是上面的S, S=4
padding=2:表示的是填充值的大小为2，也就是上面的P, P=2

假如你的图像的输入size是256x256的,由计算公式知N=(256-12+2x2)/4+1=63,也就是输出size为63x63的

```python
卷积层输出的tensor shape：（h * w * channel）。这里没有带RGB的三个通道，是因为卷积之后有一个∑(求和累加 各个通道上对应位置做累加)、relu()的两个过程
```

卷积层输出后，再经过激活函数，得到一个feature map

### 互相关运算（卷积翻转）

目的：减少一些不必要的操作或开销。互相关是一个衡量两个序列**相关性**的函数。 互相关和卷积的区别仅仅在于卷积核是否进行翻转，因此互相关也被称为**不翻转卷积**

翻转：即在两个维度（从上到下、从左到右）颠倒次序，即旋转180度（以那个边界为旋转中心？） 翻转的物理意义：对应光学中的小孔成像，即物体透过瞳孔射在视网膜上的 是物体的倒影

### 卷积核特性

- 不同的卷积核负责提取不同的特征，和卷积核是否翻转无关。
- 每一个卷积核的深度（RGB就是3通道、深度为3） 必等于 上一层输入的深度
- 1x1的卷积核作用 与 全连接层等价

#### 常用滤波器

- 高斯滤波器：对图像进行平滑去噪点

### 卷积运算变种

a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。  
b. 步长(stride)：决定滑动多少步可以到边缘。  
c. 零填充(zero-padding)：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。

#### 卷积核的超参数

1. 填充（padding）
2. 步幅（stride）

#### padding

- same padding 给平面外部补0，卷积窗口采样后得到一个跟原来平面大小相同的平面

- valid padding 不会超出平面外部，卷积窗口采样后得到一个比原来平面小的平面

### 卷积层

在图像处理中，卷积经常作为特征提取的有效方法。一幅图像在经过卷积操作后得到结果称为**特征映射**

![](./img/CNN/01.png)

### 汇聚层(池化层、子采样层、Pooling Layer)

用滤波器进行**窗口滑动**过程中，实际上"重叠"计算了很多冗余的信息，而池化操作就是去除这些冗余信息，并加快运动。

作用1：通过下采样(降采样,down sampling)，进行特征选择、降低特征数量，从而减少参数数量（减少神经元数量？） 作用2：为了缓解卷积层对位置的过度敏感性

具体方法：取区域平均或最大  
池化方法一般有一下两种：

1. 最大池化(MaxPooling)：取滑动窗口里最大的值
2. 平均池化(AveragePooling)：取滑动窗口内所有值的平均值


- 可以指定池化层的填充和步幅。
- 池化层的输出通道数跟输入通道数相同。

池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。

![](./img/CNN/pooling.webp)

#### 输出尺寸 惯用手段1：希望图片做完卷积操作后保持图片大小不变

所以我们一般会选择尺寸为3*3的卷积核和1的zero padding，  
或者5*5的卷积核与2的zero padding，这样通过计算后，可以保留图片的原始尺寸。

加入zero padding后的feature_map尺寸 =( width + 2 * padding_size - filter_size )/stride + 1

注：这里的width也可换成height，此处是默认正方形的卷积核，weight = height，如果两者不相等，可以分开计算，分别补零。

### 参数学习 卷积层误差项传递的算法

在卷积神经网络中，参数为卷积核(权重)和偏置，也是使用误差反向传播算法来进行参数学习。

### CNN应用范围

最后一层特征可以做各种任务：比如回归、分类等。

应用在图像和视频分析的各种任务（图像分类、人脸识别、物体识别、图像分割）、自然语言处理（NLP），推荐系统

### 几种典型的卷积神经网络

- LeNet-5    (早期模型，识别手写数字)
- AlexNet    (第一个现代深度卷积网络模型)
- Inception网络
- 残差网络

AlexNet与LeNet结构相似，但使用更多的卷积层、更大的参数空间来拟合大规模数据集ImageNet，它是浅层神经网络 和 深度神经网络的分界线

### 1x1卷积的作用

1. 实现跨通道的信息交互和整合。1x1卷积核只有一个参数，当它作用在多通道的feature map上时，相当于不同通道上的一个线性组合， 实际上就是加起来再乘以一个系数，但是这样输出的feature map就是多个通道的整合信息了，能够使网络提取的特征更加丰富。
2. feature map通道数上的降维。降维这个作用在GoogLeNet和ResNet能够很好的体现。举个例子：假设输入的特征维度为100x100x128， 卷积核大小为5x5（stride=1，padding=2），通道数为256，则经过卷积后输出的特征维度为100x100x256，卷积参数量为
   128x5x5x256=819200。此时在5x5卷积前使用一个64通道的1x1卷积，最终的输出特征维度依然是100x100x256，但是此时的卷积参数 量为128x1x1x64 + 64x5x5x256=417792，大约减少一半的参数量。
3. 增加非线性映射次数。1x1卷积后通常加一个非线性激活函数，使网络提取更加具有判别信息的特征，同时网络也能做的越来越深。

##### 20.卷积层和全连接层的区别

```
1.卷积层是局部连接，所以提取的是局部信息；全连接层是全局连接，所以提取的是全局信息；
2.当卷积层的局部连接是全局连接时，全连接层是卷积层的特例；
```



