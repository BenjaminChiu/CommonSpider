> 原文地址 https://blog.csdn.net/lianzhng/article/details/80652744

## 全连接层的推导

全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。

### 全连接层的前向计算

下图中连线最密集的 2 个地方就是全连接层，这很明显的可以看出全连接层的参数的确很多。在前向计算过程，也就是一个线性的加权求和的过程，全连接层的每一个输出都可以看成前一层的每一个结点乘以一个权重系数 W，最后加上一个偏置值 b 得到，即 。如下图中第一个全连接层，输入有 50*4*4 个神经元结点，输出有 500 个结点，则一共需要 50*4*4*
500=400000 个权值参数 W 和 500 个偏置参数 b。

<img src="https://img-blog.csdn.net/20160821142608048" style="zoom:67%;" />

下面用一个简单的网络具体介绍一下推导过程

![](https://img-blog.csdn.net/20160821142639705)

其中，x1、x2、x3 为全连接层的输入，a1、a2、a3 为输出，根据我前边在笔记 1 中的推导，有

![](https://img-blog.csdn.net/20160821142804582)

可以写成如下矩阵形式：

![](https://img-blog.csdn.net/20160821142838207)

### 全连接层的反向传播

以我们的第一个全连接层为例，该层有 50*4*4=800 个输入结点和 500 个输出结点。

![](https://img-blog.csdn.net/20160821143055227)

由于需要对 W 和 b 进行更新，还要向前传递梯度，所以我们需要计算如下三个偏导数。

#### 1、对上一层的输出（即当前层的输入）求导

若我们已知转递到该层的梯度![](https://img-blog.csdn.net/20160821143205990)，则我们可以通过链式法则求得 loss 对 x 的偏导数。 首先需要求得该层的输出 a<sub>i</sub> 对输入 x<sub>j</sub>
的偏导数![](https://img-blog.csdn.net/20160821143239776)

再通过链式法则求得 loss 对 x 的偏导数：![](https://img-blog.csdn.net/20160821143330662)

上边求导的结果也印证了我前边那句话：在反向传播过程中，若第 x 层的 a 节点通过权值 W 对 x+1 层的 b 节点有贡献，则在反向传播过程中，梯度通过权值 W 从 b 节点传播回 a 节点。

若我们的一次训练 16 张图片，即 batch_size=16，则我们可以把计算转化为如下矩阵形式。

![](https://img-blog.csdn.net/20160821143445694)

#### <a target="_blank"></a>2、对权重系数 W 求导

我们前向计算的公式如下图，

![](https://img-blog.csdn.net/20160821143518310)

由图可知![](https://img-blog.csdn.net/20160821143532092)，所以：![](https://img-blog.csdn.net/20160821143600382)。

当 batch_size=16 时，写成矩阵形式：

![](https://img-blog.csdn.net/20160821143703110)

#### <a target="_blank"></a>3、对偏置系数 b 求导

由上面前向推导公式可知![](https://img-blog.csdn.net/20160821143731813)，

即 loss 对偏置系数的偏导数等于对上一层输出的偏导数。

当 batch_size=16 时，将不同 batch 对应的相同 b 的偏导相加即可，写成矩阵形式即为乘以一个全 1 的矩阵：

![](https://img-blog.csdn.net/20160821143819039)

-----------------------------------------------------------------------------------------------------------------------------------

接下来再主要谈谈全连接层的意义 连接层实际就是卷积核大小为上层特征大小的卷积运算，卷积后的结果为一个节点，就对应全连接层的一个点。假设最后一个卷积层的输出为 7×7×512，连接此卷积层的全连接层为 1×1×4096。连接层实际就是卷积核大小为上层特征大小的卷积运算，卷积后的结果为一个节点，就对应全连接层的一个点。如果将这个全连接层转化为卷积层： 1\.
共有 4096 组滤波器 2\. 每组滤波器含有 512 个卷积核 3\. 每个卷积核的大小为 7×7 4\. 则输出为 1×1×4096
------------------------------------------
若后面再连接一个 1×1×4096 全连接层。则其对应的转换后的卷积层的参数为： 1\. 共有 4096 组滤波器 2\. 每组滤波器含有 4096 个卷积核 3\. 每个卷积核的大小为 1×1 4\. 输出为 1X1X4096 相当于就是将特征组合起来进行 4096 个分类分数的计算，得分最高的就是划到的正确的类别。
而全连接层的坏处就在于其会破坏图像的空间结构， 因此人们便开始用卷积层来 “代替” 全连接层， 通常采用 1×1 的卷积核，这种不包含全连接的 CNN 成为全卷积神经网络（FCN）， FCN 最初是用于图像分割任务， 之后开始在计算机视觉领域的各种问题上得到应用， 事实上，Faster R-CNN 中用来生成候选窗口的 CNN 就是一个 FCN。 FCN
的特点就在于输入和输出都是二维的图像，并且输入和输出具有相对应的空间结构， 在这种情况下，我们可以将 FCN 的输出看作是一张热度图，用热度来指示待检测的目标的位置和覆盖的区域。 在目标所处的区域内显示较高的热度， 而在背景区域显示较低的热度， 这也可以看成是对图像上的每一个像素点都进行了分类， 这个点是否位于待检测的目标上。